\documentclass{homework}

\name{Zhang Chi} % Replace (Student Name) with your name.
\id{2022010754}
\term{2024 Spring}
\course{Introduction to Theoretical Computer Science}
\hwnum{11}

%\hwname{(Name)}          % Uncomment and replace (Name) with the type of the
                          % homework (e.g, Assignment, Problem Set, etc.) if you
                          % don't want the document to be labeled as "Homework."
%\problemname{(Name)}     % Uncomment and replace (Name) with the desired label
                          % for problems created with the problem environment.
%\solutionname{(Name)}    % Uncomment and replace (Name) with the desired label
                          % for solutions created with the solution environment.

% Load any other packages you need here.

\begin{document}

\begin{problem}
  Prove that the function family
  \begin{equation*}
    \mathcal{H} = \bigl\{ h_{a,b} \mid h_{a,b}(x) = a \cdot x + b, a\in {\{0,1\}}^{k},
    b\in \{0,1\} \bigr\}
  \end{equation*}
  is a pairwise independent hash function family for range $R = \{0,1\}$ and
  domain $U = {\{0,1\}}^{k}$.
\end{problem}

\begin{solution}
We need to prove that $\forall x_1\ne x_2$ and $\forall y_1,y_2$, we have $\Pr_{h\in\mathcal{H}}(h(x_1)=y_1\wedge h(x_2)=y_2)=\frac{1}{|R|^2} = \frac{1}{4}$.

Note that:
\begin{align*}
  &\Pr_{h\in\mathcal{H}}(h(x_1)=y_1\wedge h(x_2)=y_2)\\
  =& \Pr_{h\in\mathcal{H}}(a\cdot x_1 + b=y_1\wedge a\cdot x_2 + b=y_2)\\
  =& \Pr_{a,b}(a\cdot (x_1\oplus x_2)=y_1\oplus y_2\wedge a\cdot x_2 + b=y_2)\\
  =& \sum_{i}\Pr_{b}(a\cdot (x_1\oplus x_2)=y_1\oplus y_2\wedge b=a\cdot x_2 + y_2 | a=a_i)\Pr(a_i)\\
  =& \sum_{i}1_{a_i\cdot (x_1\oplus x_2)=y_1\oplus y_2}\times \frac{1}{2} \Pr(a_i)\\
  =& \Pr_{a}(a\cdot (x_1\oplus x_2)=y_1\oplus y_2) \times \frac{1}{2}
\end{align*}
The second last equation holds because when $a_i\cdot (x_1\oplus x_2)=y_1\oplus y_2$ the event happens when $b$ happens to be the number $a_i\cdot x_2 + y_2$ which is of probability $1/2$. In the other case the event can never happen (probability $0$).

Since $x_1\ne x_2$, we have $x_1\oplus x_2\ne 0$. If we can prove that $\Pr_{a}(a\cdot x=y)=\frac{1}{2}$ for any $x\ne 0$, the proof will be done. Say the $i$-th bit of $x$ is none-zero, $x^i\ne 0$, then we can devide the sample space of $a$ (denote as $\mathcal{A}$) into two parts: $\mathcal{A}_0$ with $a^i=0$ and $\mathcal{A}_1$ with $a^i=1$. There is a natural bijection between the two parts by flipping the bit $a^i$, say $a,\tilde{a}$ only differ in the $i$-th bit. Then we have $a\cdot x = a^ix^i + \sum_{j\ne i}a^j x^j\ne \tilde{a}^ix^i + \sum_{j\ne i}a^j x^j = \tilde{a}\cdot x$. This indicates that precisely half of the $a$ in $\mathcal{A}$ will make $a\cdot x=0$ and the other half will make it $1$, so $\forall y\in\{0,1\}$, $\Pr_{a}(a\cdot x=y)=\frac{1}{2}$.

Therefore, by taking $x = x_1\oplus x_2$ and $y = y_1\oplus y_2$ we can obtain $\Pr_{h\in\mathcal{H}}(h(x_1)=y_1\wedge h(x_2)=y_2) = \frac{1}{2}\times\frac{1}{2} = \frac{1}{4}$.
\end{solution}

\begin{problem}
  \begin{parts}
    \part\label{2.a} Consider a random walk $X_{0}, X_{1}, X_{2}, \ldots$ on a chain
    of $n+1$ vertices $0, 1, \ldots, n$ with the following transition
    probabilities
    \begin{equation*}
      \Pr(X_{t} = k | X_{t-1} = j) =
      \begin{cases}
        \frac{1}{2} & \text{ if } j \in [1,n-1] \text{ and } k = j \pm 1,\\
        1 & \text{ if } j = 0, k = 1 \text{ or } j=n, k=n,\\
        0 & \text{ otherwise}.
      \end{cases}
    \end{equation*}
    Let $T_{i}$ be the expected number of steps the walk takes to arrive at the
    end vertex $n$ starting with $X_{0} = i$.
    Prove that $T_{i} \le n^{2}$ for all $i \in [0,n]$.
    \part\label{2.b} Consider the following randomized algorithm for $\TWOSAT$
    problems of $n$ variables.
    \begin{algorithmic}[1]
      \STATE{Choose an arbitrary initial assignment.}
      \FOR{$t = 1, 2, \ldots, 2n^{2}$}
        \IF{the current assignment is satisfying}
          \STATE{Accept immediately.}
        \ELSE{}
          \STATE{Choose an arbitrary clause not satisfied.}
          \STATE{Sample one of the two literals uniformly at random.}
          \STATE{Flip the value of the variable in the sampled literal.}
        \ENDIF{}
      \ENDFOR{}
      \STATE{Reject if haven't accepted.}
    \end{algorithmic}
    Use Markov inequality to show that the algorithm will find a satisfying
    solution with probability at least $\frac{1}{2}$ given a yes-instance as
    input.
  \end{parts}
\end{problem}

\begin{solution}
\ref{2.a}
Denote a walk from $X_i$ to $X_n$ as $w$, and the number of steps it takes as $|w|$. Use $w_k$ to denote the $k$-th vertex in the walk (starting from $w_0 = X_i$(starting vertex)). Then for $1\le i\le n-1$:
\begin{align*}
  T_i &= \sum_{w|w_0=X_i}|w|\Pr(w)\\
  &= \sum_{w|w_0=X_i}|w|\sum_{X_j}\Pr(w|w_1=X_j)\Pr(w_1=X_j)\\
  &= \sum_{w|w_0=X_i}|w|\bigl(\Pr(w|w_1=X_{i+1})\times \frac{1}{2}+ \Pr(w|w_1=X_{i-1})\times \frac{1}{2}\bigr)\\
  &= \frac{1}{2}\sum_{w'|w'_0=X_{i+1}}(|w'|+1)\Pr(w') + \frac{1}{2}\sum_{w'|w'_0=X_{i-1}}(|w'|+1)\Pr(w')\\
  &= \frac{1}{2}\sum_{w'|w'_0=X_{i+1}}|w'|\Pr(w') + \frac{1}{2}\sum_{w'|w'_0=X_{i-1}}|w'|\Pr(w') + 1\\
  &= \frac{1}{2}T_{i+1} + \frac{1}{2}T_{i-1} + 1.
\end{align*}
By taking $w = w_0w'$.

Specifically, we have $T_0 = T_1+1$ and $T_n = 0$. By adding the above equations together we have:
\begin{align*}
  \sum_{i=0}^{n}T_i &= T_1 + 1 + \sum_{i=1}^{n-1}\bigl(\frac{1}{2}T_{i+1} + \frac{1}{2}T_{i-1} +1\bigr)\\
  &= T_1 +\frac{1}{2}\sum_{i=2}^n T_i + \frac{1}{2}\sum_{i=0}^{n-2} T_i + n\\
  &= \frac{1}{2}(T_0 + T_1 + T_{n-1} + T_n) + \sum_{i=1}^{n-2}T_i + n.
\end{align*}
So we have:
\begin{align*}
  T_0 + T_{n-1} + T_{n} &= \frac{1}{2}(T_0 + T_1 + T_{n-1} + T_n) + n\\
  \frac{1}{2}T_0-\frac{1}{2}T_1 + \frac{1}{2}(T_{n-1} + T_n) &= n\\
  \frac{1}{2} + \frac{1}{2}T_{n-1} &= n\\
  T_{n-1} &= 2n-1.
\end{align*}
By using $T_{i-1} = 2T_i - T_{i+1} - 2$ for $1\le i\le n-1$ we can inductively prove the rest $T_i (i=0,1,2,...,n-1)$ are in following form:
\begin{align*}
  T_{n-k} = 2kn - k^2
\end{align*}
by verifying the base case $T_{n-1} = 2n-1$ and the inductive step:
\begin{align*}
  T_{n-k-1} &= 2T_{n-k} - T_{n-k+1} - 2\\
  &= 2(2kn - k^2) - 2(k-1)n + (k-1)^2 - 2\\
  &= (4k - 2k + 2)n -(2k^2 - k^2 + 2k -1 +2)\\
  &= 2(k+1)n - (k+1)^2.
\end{align*}
Thus, by $T_{n-k} - n^2 = -(n-k)^2\le 0$ we know that $T_{n-k}\le n^2$ for all $k$, which implies $T_i\le n^2$ for all $i\in[0,n]$.

\ref{2.b}
Consider the input is satisfyable CNF $\phi(x^1,...,x^n)$. Say a satisifying assignment is $x^1_*,x^2_*,...,x^n_*$. For any assignment $x^1,...,x^n$, we define the "distance" of the assignment to the answer is $d(x) = \sum_{i}(x^i\oplus x^i_*)$. Then $x=x_*\iff d(x)=0, 0\le d(x)\le n$.

When we conduct the algorithm, say the initial assignment is $x_0$ and $d(x_0) = m$. Then after each iteration step, say the current assignment is $x_k$, we randomly flip a bit in an unsatisfied clause to get $x_{k+1}$. Say we chose $C = l^i \land l^j$, and flipped $x^i_k$ or $x^j_k$. Since $C$ is unsatisfied we know that there is $x^i_k\ne x^i_*$ or $x^j_k\ne x^j_*$. Since at least one of the two variable isn't the same with $x_*$, we have at least $1/2$ chance to filp a variable that isn't same with $x_*$, and at most $1/2$ chance to flip one that is. In the former case, after flipping $x_{k+1}$ will have one less "wrong" variable, and thus $d(x_{k+1}) = d(x_k) - 1$. In the latter case, $d(x_{k+1}) = d(x_k) + 1$. Specifically, when $d(x_k)=n$, which means all variables are wrong, flipping any variable will make $d(x_{k+1})=1$.

Use vertex $X_k$ to denote all assignments $x$ with $d(x)=n-k$. By \ref{2.a}, for any initial state, in the worst case which we have only $1/2$ probability to decrease the distance by each flip, the expectation of flips to reach $d(x)=0$ (Say it's $T$) is at most $n^2$.

Since the algorithm will repeat for $2n^2$ steps, the success rate of the algorithm is $\Pr(T\le 2n^2)$. By Markov's inequality, we know that:
\begin{align*}
  \Pr(T\ge 2n^2) \le \frac{\E(T)}{2n^2} \le \frac{n^2}{2n^2} = \frac{1}{2}.
\end{align*}
Therefore, the algorithm will find a satisfying
solution with probability at least $\frac{1}{2}$ given a yes-instance as input.
\end{solution}

\end{document}
